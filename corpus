ID,Organisation,Organisation Type,Country,Euope,Month,Year,Language,Text type,URL,URL2,Text
ETHI10,Financial Times,News Media,United Kingdom,Western Europe,May,2023,EN,Position,https://www.ft.com/content/18337836-7c5f-42bd-a57a-24cdbd06ec51?sharetype=blocked,,"In its more than 130-year history the Financial Times has upheld the highest standards of journalism. As editor of this newspaper, nothing matters to me more than the trust of readers in the quality journalism we produce. Quality means above all accuracy. It also means fairness and transparency. 
That’s why today I am sharing my current thinking on the use of generative artificial intelligence in the newsroom. 
Generative AI is the most significant new technology since the advent of the internet. It is developing at breakneck speed and its applications, and implications, are still emerging. Generative AI models learn from huge amounts of published data, including books, publications, Wikipedia and social media sites, to predict the most likely next word in a sentence. 
This innovation is an increasingly important area of coverage for us and I am determined to make the FT an invaluable source of information and analysis on AI in the years to come. But it also has obvious and potentially far-reaching implications for journalists and editors in the way we approach our daily work, and could help us in our analysis and discovery of stories. It has the potential to increase productivity and liberate reporters and editors’ time to focus on generating and reporting original content. 
However, while they appear to be very articulate and plausible, AI models on the market today are ultimately a prediction engine and they are learning from the past. They can fabricate facts — this is what is referred to as “hallucinations” — and make up references and links. If sufficiently manipulated, AI models can produce entirely false images and articles. They also replicate the existing societal perspectives, including historic biases. 
It is my conviction that our mission to produce journalism of the highest standards is all the more important in this era of rapid technological innovation. At a time when misinformation can be generated and spread rapidly and trust in the media in general has declined, we at the FT have a greater responsibility to be transparent, to report the facts and to pursue the truth. That is why FT journalism in the new AI age will continue to be reported and written by humans who are the best in their fields and who are dedicated to reporting on and analysing the world as it is, accurately and fairly.  The FT is also a pioneer in the business of digital journalism and our business colleagues will embrace AI to provide services for readers and clients and sustain our record of effective innovation. Our newsroom too must remain a hub for innovation. It is important and necessary for the FT to have a team in the newsroom that can experiment responsibly with AI tools to assist journalists in tasks such as mining data, analysing text and images and translation. We won’t publish photorealistic images generated by AI but we will explore the use of AI-augmented visuals (infographics, diagrams, photos) and when we do we will make that clear to the reader. This will not affect artists’ illustrations for the FT. The team will also consider, always with human oversight, generative AI’s summarising abilities.  We will be transparent, within the FT and with our readers. All newsroom experimentation will be recorded in an internal register, including, to the extent possible, the use of third-party providers who may be using the tool. Training for our journalists on the use of generative AI for story discovery will be provided through a series of masterclasses.  Every technology opens exciting new frontiers that must be responsibly explored. But as recent history has shown, the excitement must be accompanied by caution over the risk of misinformation and the corruption of the truth. The FT will remain committed to its fundamental mission and will keep readers informed as generative AI itself and our thinking on it evolve."
ETHI11,Redaktørforeningens,Professional Organisation,Norway,Northern Europe,July,2023,NO,Position,https://m24.no/ai-christer-s-johnsen-debatt/apenhet-blir-avgjorende-for-publikums-tillit/630098,,"Editorial decisions should be made by humans. The editor has full personal responsibility, legally and ethically, for everything that is published, including generated content. Any fully automated services must be considered very carefully.  2. All use of generative AI should, as a general rule, be reviewed by humans before publication. Information must be checked against other sources. Artificial intelligence are language models, not independent sources, and are not capable of fact-checking themselves.  3. Exercise great caution when feeding unpublished material into language models such as Chat GPT, where the editors have no control over data storage. Be aware that you may otherwise jeopardise source protection or other considerations.  4. Be particularly cautious about publishing generated images that may be confused with real images of people, situations and events. The same applies to enhancement or alteration of images beyond what can be considered normal editing. Protect the credibility of documentary photography. The editorial team should have clear guidelines for the use of generated images, and exceptions should be authorised at editorial level.  5. Content generated by AI should be clearly labelled and understandable to the audience. The audience should not be in doubt about what is generated and what is editorially prepared. Be transparent about the work process where AI is involved. The name of the tool or service should be included in the labelling, and editors must ensure that its use does not violate the terms of service. Generated content must also be clearly labelled in the editorial archives.  6. Generated accessibility services such as direct translation or text-to-speech should be clearly labelled. Be conscious of how such services appear to the audience - e.g. whether the voice should belong to or resemble a real human voice or be clearly generated.  7. The editorial team should have clear rules on what generative AI can be used for and when in the journalistic process. Exceptions should be decided at editorial level. Invite discussion and be open to the possibility that the guidelines will need to change in line with developments.  8. Be generous with sharing! Remember that no guidelines are of any value if they are not known in the newsroom. Feel free to share outside the media organisation. VG and DN are among the editorial offices that have published their guidelines on artificial intelligence. This is wise. Feel free to share the guidelines with NR and the rest of the industry - and participate in the debate about both the opportunities and pitfalls of the new technology. Good luck!"
ETHI12,DJV,Professional Organisation,Germany,Western Europe,NA,2023,DE,Position,https://www.djv.de/startseite/info/themen-wissen/medienpolitik/kuenstliche-intelligenz,,"1. no substitution of human performance
Media companies must not use artificial intelligence with the aim of saving journalists' jobs. Even if tasks change through the use of artificial intelligence or are completely eliminated, people in the newsroom do not become superfluous. A responsible use of artificial intelligence in journalism always requires the irreplaceable editorial eye of journalists on the generated content to ensure its quality and to comply with journalistic due diligence.
Humans are also irreplaceable in the research and compilation of the underlying data material as well as in the programming, training and configuration of the AI application. Thus, the interaction between humans and AI applications still takes on a high value even when the level of automation of AI is high. A completely automated creation and distribution of messages must be avoided. Credibility is and remains the most important prerequisite for journalism; it can only be maintained if people bear the ultimate responsibility for the selection and presentation of content.
2 Responsibility for content
Publication of automated or AI-generated articles without the participation or effective intervention of human journalists must not take place. Editorial offices should establish clearly regulated acceptance and approval processes for journalistic content. Regardless of the automated creation and/or distribution of journalistic content by artificial intelligence, the editorial department that uses artificial intelligence always remains responsible for the content. Accordingly, the respective media company to which the editorial department belongs also remains responsible.
Media companies should appoint representatives who compare the practice of using AI in the company with the respective applicable rules, such as a future Code of Conduct on AI, and who also serve as contact persons in the event of a complaint.
3 Responsible handling of data material
The editorial team that has journalistic content created with the help of artificial intelligence using data bears responsibility for the data material on which the automatically generated content is based. The respective editorial office is responsible for ensuring that the content of the data material used is correct and complete. Data collection, preparation and processing must meet high qualitative standards. Incompleteness, distortions and other errors in the data material must be corrected immediately. When processing personal data, the relevant data protection laws must be observed.
The DJV calls on media houses to build up their own and value-based databases and supports open data projects of public authorities and state institutions. Greater independence from commercial big-tech providers is desirable. 4.
4 Transparency and labelling
Journalistic content created in whole or in part with the help of artificial intelligence must be labelled accordingly in the medium of publication. The labelling must take place in the immediate vicinity of the content and be clearly recognisable in terms of size and design.
Users shall be informed in an appropriate manner and in a suitable place which technologies are used in the automated generation and distribution of journalistic content and to what extent. In addition, users should have the possibility to find out which data was collected and used for the generation and/or distribution by means of artificial intelligence. The source of the underlying data and content must be stated, lastly also in order to enable an appropriate remuneration of the authors (cf. paragraph 8).
The DJV calls on the legislator to anchor such labelling obligations in law.
5 Responsible personalisation
Personalisation of content by means of artificial intelligence in such a way that users are shown content tailored to their interests and usage behaviour must always be carried out in a responsible and balanced manner. Filter bubbles should be avoided.
Despite personalisation, it must be ensured that users continue to be shown content curated by editorial teams. In the overall selection of content, attention should be paid to social diversity.
The criteria used by the algorithm to tailor the displayed content to the respective users should be transparently disclosed to the users. Users should be able to change the selection criteria and/or completely deactivate personalised distribution. 6.
6 Use of certified AI systems
The DJV advocates and supports the development of certifications for AI systems used in the journalistic field. The goal must be that only special, certified AI systems are used in the journalistic field that meet certain standards regarding quality, balance, non-discrimination, data and source protection as well as security. The certifications should be developed closely with politics and the relevant non-governmental organisations from the media sector.
The DJV declares its willingness to participate in such developments of certifications. 
7 Ongoing review
Media houses must continuously review the use and impact of AI in journalism with regard to the above aspects.
8. further training
The use of artificial intelligence must become an integral part of the training and further education of journalists. Media companies are called upon to create corresponding offers. In addition to the use of artificial intelligence and the resulting possibilities (e.g. when using it for research), the content of these training courses must also be to raise awareness of the misuse of artificial intelligence (e.g. for the creation of deepfakes) and to create possibilities for its discovery in everyday journalistic work.
9 Appropriate remuneration
AI applications rely on existing data and content from which they can learn and evolve. AI applications use text and data mining technologies to sift through vast amounts of existing content, usually without first obtaining the consent of the creators or paying appropriate remuneration for it. The artificial intelligence then uses and markets the third-party content or parts of it in its own name. As a result, journalists who originally produced the content become unpaid suppliers of raw data for the AI systems. This must not be allowed to happen. The DJV therefore calls on the legislator to introduce a remuneration obligation for text and data mining so that journalists can participate in the development and use of AI systems."
ETHI13,Ringier,Press Group,Switzerland,Western Europe,May,2023,EN,Guidelines,https://www.ringier.com/ringier-introduces-clear-guidelines-for-the-use-of-artificial-intelligence/,,"A conscious approach to the possibilities of artificial intelligence (AI) is highly relevant, especially for a media company. That is why Ringier has decided to introduce guidelines for the use of artificial intelligence for all companies in the Group – which is represented in 19 countries. These will be continuously reviewed in the coming months and adjusted if necessary.

The Ringier guidelines for dealing with artificial intelligence (AI) include the following points:

    The results generated by AI tools are always to be critically scrutinized and the information is to be verified, checked and supplemented using the company’s own judgment and expertise.
    As a general rule, content generated by AI tools shall be labeled. Labeling is not required in cases where an AI tool is used only as an aid.
    Our employees are not permitted to enter confidential information, trade secrets or personal data of journalistic sources, employees, customers or business partners or other natural persons into an AI tool.
    Development codes will only be entered into an AI tool if the code neither constitutes a trade secret nor belongs to third parties and if no copyrights are violated, including open source guidelines.
    The AI tools and technologies developed, integrated or used by Ringier shall always be fair, impartial and non-discriminatory. For this reason, Ringier’s own AI tools, technologies and integrations are subject to regular review.

Ringier CEO Marc Walder: “We are convinced that through responsible interaction between humans and machines, based on clear AI rules, our products and processes can be further improved. By introducing these rules, we are taking on international responsibility to leverage the positive aspects of AI while minimizing the risks. In this way, Ringier ensures that AI is used transparently and in line with its values.”"
ETHI14,Conseil de déontologie journalistique et de médiation,Press Council,France,Western Europe,July,2023,FR,Principles,https://cdjm.org/journalisme-et-intelligence-artificielle-les-bonnes-pratiques/,,"Any sufficiently advanced technology is indistinguishable from magic"". Recent advances in artificial intelligence (AI) fully verify this quote from Arthur C. Clarke. Over the past few years, the media have been seizing on the new AI tools available, which endeavour to reproduce the activity of the human brain thanks to ever more advanced algorithms. They can be used at all stages of journalistic work: firstly, when gathering and processing information, then when producing the content itself, and finally when publishing or broadcasting it.  But since 2022, this development has accelerated sharply. Publicly accessible image-generation tools such as Midjourney can be used to create increasingly realistic visuals from text descriptions. ChatGPT, the conversational robot launched by OpenAI, has enabled everyone to test for themselves the progress and limitations of large language models (LLMs). This sudden popularity has boosted the sector, and announcements from digital giants and new players alike are coming thick and fast. Because the potential and real risks of these services are still poorly understood, they are arousing the enthusiasm of the optimists as much as the fear of the pessimists.  For the media as a whole, the rise of artificial intelligence raises a series of highly sensitive questions. Will it put out of work some of the professionals whose tasks can now be carried out by simple software? How can we ensure fair remuneration for authors and publishers whose content feeds the generative AI corpus, and who may legitimately demand a share of the value thus created? Can we really rely on technologies whose biases have often been denounced, accentuating the discrimination suffered by some sections of the population? Can they undermine our democracies?  This document focuses on just one of these ethical challenges, that of journalistic ethics. It aims to establish how the principles of the reference charters, the oldest of which is over a century old, can be applied in this new and uncertain context. While in some cases it draws red lines that should not be crossed and in others it describes good practice that should be respected, it does not seek to deprive newsrooms of tools that are useful on a day-to-day basis.  Indeed, innovation specialists have been quick to identify how journalists can free themselves from tasks that can now be automated, saving time for work with higher added value. In the final analysis, this is also the aim of the working group of the Council for Journalistic Ethics and Mediation (CDJM) behind this recommendation: to ensure that advances in AI are used to deliver quality information to the public.  Precautions and red lines  The acceleration and optimisation of work made possible by artificial intelligence tools must not be used as an opportunity to call into question the principles of ethical charters, whether in terms of the requirement for accuracy and truthfulness of facts, the necessary independence of editorial work, respect for the dignity and privacy of individuals and the protection of sources, or the imperative not to fuel hatred and prejudice. Whether a journalist is simply a user of these emerging services or helps to configure and develop them in-house, he or she must ensure that the rules underpinning the ethics of journalism are respected.  This document looks at the possible uses of artificial intelligence by journalists and newsrooms, dividing them into several levels, following the example of the European Union institutions, according to the degree of risk they represent in terms of compliance with professional ethics.  I. Low-risk uses  These uses have no impact on the information provided to the public. They may be discussed internally, but do not necessarily need to be notified to the reader, listener or viewer.  This includes, for example  - correcting spelling and grammar before publication  - automated translation of texts for prior documentation purposes  - transcription of recordings (speech to text) to extract relevant  relevant quotations  - the creation of drafts for publications that you wish to distribute  on social networks  - generating ideas for SEO-optimised titles  - help you identify and select information circulating on  social networks  - analysing and summarising complex issues during preparatory research  - verifying assertions (fact checking) in real time using in-house tools.  If they use these AI services, journalists always check and validate the elements generated before they are used in any production.  II. Moderate-risk uses  These uses may have an impact on the information published or broadcast. They must remain under the direct responsibility of publishers and journalists, and be explicitly indicated to their audience.  A. The public must be informed when an AI tool is used in the production of broadcast or published content  This statement must appear whenever the content published or broadcast is not essentially the result of the activity of human brains.  This transparency gives the public a better understanding of the conditions under which the content they see is produced. It also establishes a necessary distinction between what is the fruit of the labour of real people and what is the product of a series of algorithms.  Furthermore, any imperfections due to automation must never significantly alter the information delivered.  This includes, for example  - automated translation of articles published in a foreign language  - advanced voice synthesis (text to speech) for automatic oral reading of texts  - automated updating of content based on new data available  available data  - classification of articles by keywords  - summaries of articles or audiovisual elements  - generation of simplified French versions  - factual summaries of election results  - technical reports of sporting events  Generally speaking, content generated directly by these tools can only be published or broadcast automatically if journalists have been able to identify and control the information originally used and the processing steps applied to it. In this way, they control its editorial relevance.  The data sources must be mentioned; the names of the tools used and any queries used (prompts) may also be indicated.  B. When visuals generated by AI tools (drawings, illustrations, artist's views, reconstructions, etc.) are used, there must be no doubt as to their artificial nature and the audience must be informed of this.  The origin of any synthetic image published or broadcast must be made clear. In the written press (print or digital), the caption should indicate something like ""image generated by an AI"". If the visual is inserted into a video sequence, a visible mark should indicate this throughout the shot, similar to the ""hidden camera"", ""discreet camera"" or ""reconstruction"" references often used in television. The image generation tool used and the command text used (the prompt) can also be indicated.  C. Journalists and publishers ensure that ethical rules are respected when configuring or developing AI tools used internally  Journalists are involved in the teams responsible for carrying out this type of project and ensure, from the design phase through to delivery, that the operation chosen does not call into question compliance with the principles set out in the ethical charters common to the profession and/or specific to the media.  D. AI tools used to personalise the information delivered must not deprive teams of control over the editorial line  Based on browsing data and other personal information, AI tools can modify the experience of users of digital services. For example, visitors to a news website may be offered content tailored to their profile (age, location, favourite topics, etc.), or even a tailor-made hierarchy of information. Publishers and editorial teams must ensure that these systems do not conflict with the editorial line and editorial choices made in the treatment of news.  III. Uses to be avoided  These uses are intrinsically incompatible with respect for journalistic ethics.  A. Journalists do not use AI tools to generate images, sounds or videos whose realism risks misleading the public or leaving it in a state of ambiguity, by presenting it with information that is contrary to the reality of the facts.  With or without artificial intelligence, publishing a fake is a major ethical error. It is not enough to add, in a caption or commentary, that the photo published is a synthetic image or to mention, in the credit, the tool used to produce it: this information may in fact escape a large proportion of readers or viewers.  By way of exception, such images can be used to illustrate subjects relating to the creation and circulation of these false images. In this case, it is good practice to add a visible and explicit reference to the creation, in order to limit the risk of misleading the public, particularly in the event of re-use.  B. Content generated in its entirety by AI tools cannot be published or distributed without editorial control  Publishers and editorial departments are responsible for this content, in the same way as for human productions; it must therefore be distributed exclusively under the supervision of journalists, after proofreading and validation stages that guarantee the reader the veracity and quality of the information published or distributed.  IV. Other recommendations  Beyond the immediate ethical rules, the maintenance and promotion of ethical journalism in the face of the uses of AI also require proactive attitudes in editorial offices.  The CDJM believes that publishers and journalists should strive ...  ... to improve the queries (prompts) used in generative AI tools  Chatbot-type interfaces have helped to popularise large language models (LLMs) and given rise to a host of new uses within newsrooms. But the quality and reliability of the texts produced depend largely on the formulations used by the user as instructions - the prompt. Editorial departments wishing to make use of these tools need to be trained in the correct use of prompts, and can regulate their use by developing internal rules in this area (prompt engineering).  ... make better use of the work of photographers and photojournalists  With the proliferation of images generated by AI tools and the degree of realism they achieve, there is an increasing risk that the public will take as true what is false, but also, conversely, that they will take as virtual what is very real. Respect for professional ethics means giving priority to what is real, in this case the work of photojournalists and other witnesses on the ground. To this end, it is important to make captions more explicit and credits more visible, by systematically indicating the place and date the photo was taken. Newsrooms will also be attentive to projects aimed at technically authenticating photos distributed to the public, such as the Content Authenticity Initiative or the Pix T protocol.  ... learn about new AI tools in order to assess their benefits and limitations  Restoring reality for an audience also involves mastering the tools used to do so. The CDJM encourages journalists and newsrooms to train in AI tools, to better understand what they produce or to identify their risks, biases and limitations.  ... to exchange with specialists in the field so that the challenges facing the profession are taken into account  Similarly, the CDJM considers that the ethical challenges of new technologies are such that it is important for journalists to be involved in the design and development of AI tools intended to assist them in their work of informing the public."
ETHI15,Dagens Næringsliv,News Media,Norway,Northern Europe,June,2023,NO,Guidelines,https://www.dn.no/teknologi/retningslinjer-for-redaksjonell-bruk-av-kunstig-intelligens-ki-i-dagens-naringsliv/2-1-1463963,,"Dagens Næringsliv works according to the Vær Varsom poster's rules for good press etiquette and everything that is published is quality assured by the editors. Editorial responsibility and press ethics are platform and technology neutral.

Our ambition is to use artificial intelligence to make Dagens Næringsliv an even better newspaper by improving the journalistic workflow and presenting our journalism in the best possible way. To succeed, we must experiment, evaluate and learn about new technology.

With this as a background, Dagens Næringsliv has drawn up these guidelines for editorial use of artificial intelligence.
Generative language models (Ex. ChatGPT, Bard etc.)

     DN's journalists can use language models as a tool, but the text must always be processed by a human and go through ordinary source work.
     Examples of use are; proofreading, translation, transcribing, title suggestions, summarizing, idea development and metadata generation.
     We do not publish unedited text written by generative artificial intelligence. The exception is when the language model itself is a relevant journalistic point. It then specifies which AI tools have been used and in what way.
     We protect source protection and do not share sensitive material or personal information.

Image and video generators (Ex. Midjourney, Dall-E)

     DN protects the credibility of journalistic photography, and does not manipulate images with artificial intelligence or other technology.
     We do not publish AI-generated images, or video, that could be mistaken for real photography.
     The exception is in the case of mention of the technology or related news items/problems.
     We can use AI-generated illustrations. This must always be approved by editorial management.

     The illustrations must be clearly labeled with the source and relevant background information. They are labeled to avoid reuse.

     Rights around AI-generated images are still unclear, and the technology should therefore be used with caution.

Other uses of artificial intelligence

     DN collaborates with Amedia on the development of language models for news written in Norwegian. This is currently used to recommend relevant articles in the stock exchange service Investor, in our wine search and at the bottom of articles.
     Algorithms contribute to the placement of content on certain floors on DN.no. This means that the content may look different to different visitors.
     DN's automated journalism (e.g. the housing robot) is quality assured through rules and article templates. It is therefore published without human approval, and is labeled as automatically generated content.

Here you can read more about how DN Media Group AS processes your personal data.
Journalism and artificial intelligence

     Journalism must be critical and investigative. This also applies in the face of new technology such as generative AI.
     Text and image generators are useful tools for actors who want to spread fake news and disinformation. Editor-controlled journalistic media are an important counterweight to this. It is therefore essential to maintain editorial integrity and credibility in the face of new technology.

The guidelines are continuously updated as technology develops."
ETHI16,De Volksrant,News Media,The Netherlands,Western Europe,Mei,2023,NL,Guidelines,https://www.volkskrant.nl/kijkverder/Documenten/volkskrant-protocol.pdf?referrer=https://www.startpage.com/,,"The Volkskrant views artificial intelligence (AI) as a tool, never as a system that can replace the work of a journalist. The content of the Volkskrant is created by human editors, reporters, final editors, photographers and illustrators. In everything the Volkskrant makes, there should be no misunderstanding about what is real and what is true and what is not. The editors therefore do not publish journalistic work that has been generated by artificial intelligence. De Volkskrant also wants to be completely transparent about how information has been gathered and which sources underlie it. behind it. Journalistic work generated by AI systems does not offer this transparency. De Volkskrant is made by specialised editors, who work according to the Volkskrant protocol and work reliably, independently, with integrity and as objectively as possible want to be. Generative AI too often contains errors ('hallucinations') or biases ('bias'), and it is usually unknown what data the systems were trained with. Therefore, the reliability of sources on which the system is trained cannot be verified. Rights holders have often not given permission to use their material for training. With both text and images, the Volkskrant explicitly weighs in that employees create something new and depend on their creative ability in their livelihood. The exception to the above is when the purpose of a publication is to show how an AI system works. If, in the context of such a publication, something is 'written' or generated by artificial intelligence, the Volkskrant always explicitly mentions this, indicating where any errors are in the generated work. Such use of generative AI is done only with the permission of chiefs and chief editors. That said, artificial intelligence can serve as a tool for final editing, research and as a tool. For example, to transcribe audio recordings of an interview, to have texts translated or to have stories narrated by synthetic voices so that readers can listen to them. Thereby, the decision whether or not to use a service, and the generated, is always made by a human. Nothing generated by AI should be used under the name of the Volkskrant without a human's approval. And it must always be clear that these are not real votes. When the Volkskrant itself uses artificial intelligence as a tool to conduct journalistic research, for example filtering specific information from a large data set, the editors are transparent about their methods. The editors provide an account of the method used and the journalistic choices made. Artificial intelligence is technology that can influence journalistic work in ways we cannot predict. De Volkskrant follows developments with critical interest. Where necessary, this protocol will be adapted."
ETHI17,Dagbladet,News Media,Norway,Northern Europe,June,2023,NO,Guidelines,https://www.dagbladet.no/nyheter/redaksjonelle-retningslinjer/66579649,,"Editorial guidelines for AI in Dagbladet 

Dagbladet must constantly make judgemental assessments of the use of artificial intelligence, as the technology is constantly changing. In general, Dagbladet's employees adhere to the following guidelines to ensure that the technology is used in a responsible manner:

    Dagbladet encourages all employees to be open and curious when encountering AI. Learning more about how the tool can be utilised in an ethically responsible manner will be a great advantage.
    All use of generative AI must be manually approved before it is published. This means that a human must monitor and quality assure the results from the AI tools.
    Dagbladet will be open and transparent about the use of AI. All AI-generated content must therefore be clearly labelled.
    Exercise great caution when sharing unpublished material with AI services. Content to be shared must be ready for publication and not contain sensitive information or information that requires source protection.
    Privacy and protection of sources must also be safeguarded when dealing with AI services. Therefore, as a general rule, sensitive personal data or source information must not be shared.
    As a general rule, Dagbladet shall not use AI to create photorealistic images or video. If Dagbladet exceptionally uses AI-generated images, they must not be confused with real photographs or videos and be clearly labelled for the reader or user."
ETHI18,VG,News Media,Norway,Northern Europe,April,2023,NO,Guidelines,https://www.vg.no/informasjon/redaksjonelle-avgjorelser/188,,"Editorial guidelines for AI in VG  AI technology is changing rapidly, and VG's guidelines will therefore be reassessed on an ongoing basis.      The first main rule is that VG's journalists shall relate to the use of text, video, images, sound and other content created using AI (generative AI) in the same way as other sources of information; with open curiosity and caution.     The second main rule is that until further notice, all use of generative AI must be manually approved before publication.     The third main rule is that VG shall not use AI to generate photorealistic images. The use of AI-generated content must never undermine the credibility of the journalistic photograph. VG shall avoid using AI to enhance/edit photographs and video, beyond traditional image processing and cropping.  VG may, however, use AI to generate illustrations/graphics/modelling. This must be clearly labelled and, until further notice, specifically approved by the editor.      The fourth main rule is that AI-generated content must be clearly labelled and in a way that is understandable.     The fifth main rule is that VG's journalists should only share material that has been approved for immediate publication on VG's platforms with AI services.  Source information, personal data or sensitive information should not be shared with AI services without authorisation from the head of department, editor or security manager."
ETHI19,Aftonbladet,News Media,Sweden,Northern Europe,May,2023,SE,Principles,https://www.aftonbladet.se/omaftonbladet/a/76ydy9/aftonbladets-ai-policy-sa-forhaller-vi-oss-till-den-nya-tekniken,,"General policy for AI

Aftonbladet is responsible for everything we publish on the site, including material that is produced using, or based on, AI or other technology. All material published has been reviewed by a human and falls under our publishing authority.

    Aftonbladet's journalists can use AI technology in their work to produce material. This can, for example, involve doing research, predicting ideas or giving suggestions for headlines.
    However, everything we publish is produced, edited and fact-checked by a human. In the rare cases where we publish AI-generated material, this will be clearly stated.
    All images, both moving images, still images and illustrations, that are published on Aftonbladet.se are taken by our photographers/image agencies or produced by people. In cases where we publish AI-generated images, this will be clearly stated. As before, we do not manipulate or distort news images, either with the help of AI or other technology.
    Aftonbladet.se's front page is to some extent controlled by algorithms that can affect the placement of certain content. Some areas of the site are also partially personalized, which means that parts of the front page may look different to different visitors.

How should Aftonbladet's employees relate to AI?

    Aftonbladet must be at the forefront of new technology and we encourage our employees to use it in their work.
    However, we treat and relate to AI technology in the same way as to all other technology and information: with critical eyes and with caution.
    We protect source protection and do not feed external platforms such as ChatGPT with sensitive or proprietary information.

The policy may be updated in line with developments."
ETHI20,Heidi.news,News Media,Switzerland,Western Europe,April,2023,FR,Principles,https://www.heidi.news/cyber/la-redaction-de-heidi-news-prend-position-sur-l-usage-des-intelligences-artificielles,,"General principles 
The rise of generative AI does not call into question the ethical principles of the profession, as set out in the 1971 Munich Declaration, the 2019 Tunis Declaration, and the Swiss Declaration of the Duties and Rights of Journalists.
Editorial staff may use AI to facilitate or improve their work, but human intelligence remains at the heart of all our editorial production. No content will be published without prior human supervision.
Heidi.news supports the work of journalists, writers, photographers and illustrators. Our media does not intend to replace them with machines.
Synthetic texts
Every article published is signed by one or more journalists, who remain guarantors of the truthfulness and relevance of the information it contains.
Artificial intelligence can assist journalists in refining raw data in the same way as software such as Excel, and as an aid to writing articles, like online thesauruses or automatic spellcheckers.
AIs are tools, but not sources of information.
Synthetic images
The editors limit themselves to using synthetic images for illustrative purposes, not for information, so as not to confuse real-world events.
Heidi.news will not publish synthetic images that could be mistaken for photographs, except for educational purposes, when the image in question is already public.
All published synthetic images will be visibly marked to indicate their origin. The caption will mention the AI model used and the main instruction given to it.
Heidi.news Explorations (major stories, investigations, reports) are not intended to be illustrated by synthetic images, unless these creations have been thought up by an artist using AIs under his own responsibility."
ETHI21,Raad voor de Journalistiek,Press Council,Belgium,Western Europe,NA,2023,NL,Ethical Code,https://www.rvdj.be/nieuws/nieuwe-richtlijn-over-het-gebruik-van-artificiele-intelligentie-de-journalistiek,," Artificial intelligence can play a role in the cooking, editing, production and dissemination of news items, such as articles, reports, illustrations, infographics, etc. Editorial choices play a role in such partially or fully automated processes. Those choices must comply with the principles of the Code.

The editors are responsible for these editorial choices, with final responsibility resting with the chief editors. The chief editors guarantee the principles of the Code when developing systems that are partly or entirely driven by artificial intelligence. It monitors the application and implementation of these principles vis-à-vis system developers. It is responsible at all times for the provision of information, regardless of how it is produced and regardless of the channel or form in which it is provided.

Editors communicate transparently about automated news production and personalisation of news offerings so that it is clear to users when news items have been created or selected based on artificial intelligence.

    The editors will indicate when a news item or part of the information offer is partly or fully produced on the basis of automated processes and, as far as possible, refer to the sources on which the item is based.

    The editorial indicates when an element of the information offer is selected or diversified on the basis of profile or media consumption."
ETHI22,The Guardian,News Media,United Kingdom,Western Europe,July,2023,EN,Guidelines,https://www.theguardian.com/commentisfree/2023/jul/27/guardian-observer-editorial-code-update-journalism,,"Generative AI is a broad label describing any type of artificial intelligence that uses unsupervised learning algorithms to create new digital text, images, video, audio or code. Prominent examples include ChatGPT and Midjourney and the technology is rapidly becoming more sophisticated and accessible. Material created using generative AI raises significant issues around bias, ownership, plagiarism and intellectual property rights. Most importantly for journalism, it is not reliable or consistent, and tends to introduce errors and inaccuracies in unpredictable ways. AI systems should not be used to generate text or images intended to be directly inserted into published journalism outside of exceptional and specific circumstances. Any exceptional use must be explicitly approved by the relevant senior duty editor and must be clearly signalled to readers on the article itself."
ETHI24,ANP,Press Agency,The Netherlands,Western Europe,NA,2023,NL,Principles,https://www.villamedia.nl/artikel/persbureau-anp-stelt-leidraad-met-vangrails-op-voor-inzet-van-kunstmatige-intelligentie,https://drive.google.com/file/d/1-3sAJtkOJrdIGw-gZFqYDEGQQNw13e0U/view?usp=sharing,"Principles of deployment AI Drafted by chief editors, approved by chiefs and editorial board 
Just like previous automatisations (the telegraph, the telex, the computer, the fax, the internet, the telephone, search engines, social media...), artificial intelligence ('AI') will change our journalistic work in ways we, as ANP editors, cannot yet foresee: some innovations significantly facilitate our daily work and take us rapidly forward both journalistically and professionally, others may touch on our basic principles including our commitment to quality and reliability. 
Therefore, we hereby set out basic principles on how the ANP editorially handles AI or derived artificial editorial applications including but not limited to so-called 'generative AI', 'templating' or 'text-to-speech' and 'speech-to-text'. Simply put, this is the line we follow. This is a living document that can be modified by the editor-in-chief as developments demand. 
As with everything, in our thinking and work we first refer back to the Editorial Statutes. These state, among other things, that the purpose of ANP b.v. is ""to maintain a completely impartial and independent agency for the objective provision of domestic and foreign news reports to the Dutch media"". For the journalistic policy, the articles of association keep us on our toes, our maxims include ""no direct influence by anyone, neither from outside nor from within, other than in the manner provided for in this Statute"", ""impartiality and meticulousness in the representation of verifiable facts and opinions"" and ""completeness and balance in the selection from relevant sources"".
This is what we have to abide by. In line with this, the following AI principles have been drawn up by the chief editors and endorsed by the chiefs and editorial board. Violation of these is considered contrary to our basic journalistic principles and comparable to committing plagiarism. 
- We rely on the editorial statutes, the law, the basic democratic principles prevailing in the Netherlands and the mores of the journalistic profession at all times when using AI, regardless of the possibilities offered by technical developments, in the interest of customers, our editorial standards and journalistic social authority. We are convinced that the ANP is best served by this. 
- As we look at society, every member of the ANP editorial team looks at AI and related systems: full of wonder, inquisitive, critical and open to developments. 
- With this, we see many opportunities and can be inspired by the computer (or by AI) for headlines, backgrounds, story ideas or sources to be accessed, for example. Also - for example - recorded texts can be written out automatically and other support services applied. The number of assistance options is bound to increase further. It is up to editors whether it is relevant and useful whether AI applications are used in editorial or journalistic productions. Simply put: AI is, as it looks, a tool and not a substitute. 
- We can use AI or similar systems to support final editing, provided a human does a final check afterwards.
- We continue to innovate on the basis of 'cautious experimentation'. We want to actively pursue innovations and efficient work processes with an open mind, but do not show recklessness in doing so. Editorial innovations on journalistic productions for which the chief editors are responsible are presented to the public only if they comply with the principles of this document and the quality requirements applicable within the ANP editorial team. 
- We publish texts, images, graphics or audio generated in any way by computer (or by AI) only with the permission of chiefs and chief editors. In this, we are both investigative and cautious because Al texts may look promising as much as they may contain errors Challucinations, and reaffirm biases (bis'). Moreover, with computer-generated content, it is complex to guarantee the reliability of facts presented as true. In this consideration of publication, we include how the AI system in question arrived at the information. 
- In our production chain, we stick to the already prevailing line human>machine>human where thinking and decision-making begins and ends with humans, possibly assisted by computers in the intermediate production. We therefore do not use computer (or AI) generated content, even as source material, without checking this information by a human. After all, we remain, each of us remains, responsible for ANP's reliability. We start with (and end with) the facts. 
- We are open to our customers about our working methods and the use of Al or other technical systems. As a journalistic service provider we strive for transparency, in that line we state the extent of Al use where we as editors deem it appropriate. Naturally, we are always available to customers for questions or explanations. 
If there are questions to which the above principles do not provide an answer, we will consult with you. This way we avoid surprises afterwards."
ETHI25,SVT,Public Broadcaster,Sweden,Northern Europe,June,2023,EN,Guidelines,https://www.publicmediaalliance.org/wp-content/uploads/2023/07/Policy-generative-AI-at-Sveriges-Radio-Draft-English-July-2023.pdf,,"Swedish Radio is actively exploring how AI can enhance our offerings to the audience and streamline our operations. AI and automation have been used for several years for podcast recommendations and news curation, among other things. Now, powerful generative AI models are being launched, which we see as having significant opportunities and risks – in terms of editorial, legality, and security – for Swedish Radio. These technologies exist in both new external services and increasingly in systems already used in our daily media production. These are company-wide guidelines for AI-generated content and the use of generative AI models at Swedish Radio. The guidelines apply to the use of generative AI in the entire operation – this first version focuses on the editorial aspects and will be supplemented with further guidelines for more operational needs. The policy is written by the AI Council and decided by the DG (2023-07-06). Definitions: Generative AI refers to technologies that can create new, unique content (such as text, images, audio, video, and code) based on the content used to train them. (Source: EBU) Artificial intelligence is a broad and evolving umbrella term. An AI system is a machine-based system that is capable of influencing the environment by producing an output (predictions, recommendations or decisions) for a given set of objectives. (Source: OECD) Guidelines for generative AI at Swedish Radio: • All use of AI-generated content to the audience must adhere to the same editorial principles as any other journalistic content. Swedish Radio's program rules, based on our broadcasting remit, apply. These rules, along with other guidelines and editorial practices, are compiled in the SR Public Service Handbook. • All direct use of content created using generative AI models to the audience must be approved by the legally responsible publisher (RP) for the specific publication. The RP may consult the AI Council for advice. The content must always be reviewed by a human editor to ensure that it meets our journalistic requirements and editorial standards. • All AI-generated content used to the audience, following approval by the RP, must be clearly marked in accordance with the current recommendations from the AI Council. For example, it should be clearly indicated if an image is AI-edited in all contexts where it is displayed. The phrasing may vary on a case-by-case basis depending on the employed tool, targeted audience, and area of use. We should be open and transparent about our use of AI and, for example, respond to questions from the audience. • Any services or program that include generative AI in permanent, ongoing production must be actively approved by the management. We should exercise particular caution regarding AI- generated, synthetic content – such as cloned voices and AI-generated images – in our news operations. Date: 230706 SR AI Council, Version 1, ENGLISH (AI-assisted translation) 2 • We should be vigilant about AI-generated content published or disseminated by other parties, such as ""deepfakes"" (i.e., AI-manipulated content) intended to mislead or spread disinformation. We must check and evaluate the facts ourselves when citing other sources, aware that it may be AI-generated content. • We should be aware that bias/prejudice may be inherent in the models, and editorial/legal considerations may be needed regarding objectivity and impartiality, regardless of the stage of the production process in which generative AI is used. • Each department considering the use of generative AI has an obligation to familiarize itself with the rights and obligations associated with the use of content created with the specific model. This includes ensuring that the generative AI service, as expressed in its terms of use, specifies who owns any eventual rights to the created content and how it may be used in media production. The AI Council can be consulted for support and guidance. • No confidential or proprietary information should be shared in external AI services via prompts (instructions). Any information we share there should be considered public. If we share information obtained from a third party with an external AI system, the department is responsible for ensuring that we have adequate rights to do so. • For security reasons, we should not use our SR email addresses to create accounts in external AI services. Further recommendations on information and IT security can be read by SR employees on the intranet. A newsroom or employee considering the use of generative AI in our journalistic production should start by considering the questions in this checklist: • Could using AI-generated content in this case negatively affect our credibility? • Is the responsible publisher involved in the discussion? • What do we know about the AI model and its training data? Can it reproduce biases? • Can we ensure that our content/usage does not infringe on anyone's rights? • Are we familiar with Swedish Radio's recommended formulations for explaining to users that the content is AI-generated? A department or employee considering the use of generative AI in other areas of operations should start by considering the questions in this checklist: • Could using AI-generated content in this case negatively affect our credibility? • Does the information we intend to share with the AI model include proprietary information or personal data? • Who owns the response? Are we allowed to use it? Are there prohibited use cases in the terms? • Have you carefully read and reviewed the factual content of the AI-generated responses? • Have you discussed this specific use case with your department and your supervisors?"
ETHI27,Impress,Press Council,United Kingdom,Western Europe,February,2023,EN,Ethical Code,https://www.impressorg.com/standards/impress-standards-code/our-standards-code/,,"The Code requires publishers to take reasonable steps to verify
the accuracy of the information obtained from third-party
sources; for example, blogs, social media posts, and YouTube
videos. The publisher should:
(a) be aware of the use of artificial intelligence (AI) and
other technology to create and circulate false content (for
example, deepfakes), and exercise human editorial oversight
to reduce the risk of publishing such content;
(b) be aware of the use of AI by news distributors to generate,
curate, rank and circulate news;
(c) exercise editorial oversight to ensure the accuracy of any content produced by an AI system;
The use of artificial intelligence (AI) may bring several benefits to a publisher, although AI can also create issues with accuracy (see the guidance to Clause 1) and transparency. When using AI to generate, publish and disseminate news, publishers
must exercise editorial oversight to ensure that their use of it is transparent. Publishers should prominently label content that has been recommended to people by automated systems based on their individual behaviour and data and provide
them with easily accessible options to opt-out from the same.
In addition, publishers should disclose what data they hold about people and how it has been used to make targeted recommendations.
Publishers should be aware that people are entitled to complain to Impress about AI-generated content in the same way that they do for human-generated content. Publishers should make it clear that they are responsible for the content
where it has been produced in this way. Finally, publishers should clarify what editorial mechanisms or policies they have in place regarding their use of AI.



"
ETHI28,DPA,Press Agency,Germany,Western Europe,April,2023,DE,Guidelines,https://innovation.dpa.com/2023/04/03/kuenstliche-intelligenz-fuenf-guidelines-der-dpa/,,"The dpa uses AI for various purposes and is open to the increased use of AI. AI will help to make our work better and faster - always in the interest of our customers and our products.
The dpa only uses AI under human supervision. The final decision on the use of AI-based products is made by a human. We respect human autonomy and the primacy of human decisions.
The dpa only uses legitimate AI that complies with applicable law and legal requirements and that meets our ethical principles, such as human autonomy, fairness and democratic values.
The dpa uses AI that is technically robust and secure to minimise the risks of error and misuse. Where content is generated exclusively by AI, we make this transparent and explainable. The responsibility for all AI-generated content always lies with a human being.
The dpa encourages all employees to be open and curious about the possibilities of AI, to test tools and make suggestions for their use in our workflows. Transparency, openness and documentation are crucial."
ETHI29,Groupe Les Echos-Le Parisien,Press Group,France,Western Europe,May,2023,FR,Principles,https://www.cbnews.fr/medias/image-groupe-echos-parisien-s-engage-face-intelligence-artificielle-generative-76799,https://pressroom-lesechos-leparisien.com/ia-les-engagements-du-groupe-les-echos-le-parisien/,"The engagements: 

Use of text generators

- We do not intend to publish editorial content generated in whole or even in part by an IAG without editorial and human supervision. However, we reserve the right to do so for illustrative purposes (as part of a ChatGPT Survey for example). In this case, we will explicitly mention the origin of the content.

- We reserve the right to use the IAG as a tool for enrichment, research or synthesis to help and nourish the work of journalists. For example, a journalist can use these tools as he would with a search engine, but he will always have to go back to his own sources to guarantee the origin of his information.

Use of image or video generators

- We do not intend to publish images or videos exclusively generated by the AGI without editorial and human supervision. As with all editorial content, we wish to protect ourselves against any risk of error, violation of copyright, but also to support the work of artists, photographers and illustrators.

- As for the texts, we will make an exception if visuals generated by the AGI are used to illustrate: articles on the AI, or if the creative process of the illustrator uses AGI. In any case, we will make sure to specify its origin explicitly for the reader.

- In the same way, we can, as for the texts, use the IAG as a tool to help with the synthesis of videos, in particular to facilitate the distribution of these on different platforms, and subject to human supervision and editorial."""
ETHI30,STT,Press Agency,Finland,Northern Europe,June,2023,FI,Guidelines,https://stt.fi/ajankohtaista/toimituksen-tekoaly/,,"The guidelines will be updated and refined as technologies evolve and as we learn more about how to use them in our delivery work.  The use of artificial intelligence is growing rapidly. Journalists need to understand AI both as a news topic and as a tool for news work.      It is therefore worth experimenting, asking questions and increasing understanding. For example, you can ask ChatGPT for ideas and suggestions for interesting angles and headlines, or try its ability to version stories. Don't let the AI decide for you, but always assess the usefulness of the answers yourself.     STT's guidelines for using AI to create content  Read STT's article on the use of AI in Finnish companies Every piece of information should be verified by a trusted source  Current AI text-generating applications, such as ChatGPT, produce a lot of factual errors, such as wrong dates, made-up names and events that are simply not correct.      The ChatGPT homepage warns that the tool may occasionally produce incorrect information. In fact, I say that it may occasionally produce inaccurate data. However, that does not mean that it is not worth testing the tool to see what it can do, despite its limitations.     Maija Paikkala, Business Developer  A mobile phone is placed on top of a computer keyboard to run the ChatGPT text-to-speech application.  Due to errors, STT does not use AI for data mining. The sources used by AI are often obscure, making it problematic to use in editorial work.      The STT's information gathering must be intact and as transparent as possible, and therefore AI applications should not be used. Sources must be known to the editorial staff even if they cannot be directly mentioned in the story. If you wish, you can try to find out what ChatGPT knows about an issue, but for each individual issue it reports, a reliable source must be found before it can be published.     STT guidelines on the use of AI for content creation  Read our previous blog post on the use of AI in journalism Source criticism is always more important  Source criticism is one of the most important tools a journalist has, and its importance will only increase with the rise of content AI. It can be difficult to identify AI-generated text or images, even though tools are already available. Reliability of sources and verification of information from other sources are particularly important.  AI also uses reliable sources, also in reliable ways. For example, AI can be used in research.      In such cases, the journalist must stop to consider how the use of AI has affected the outcome. It is also often a good idea to make a phone call and ask how AI or other software has been used.     Minna Holopainen, editor-in-chief of STT  The use of AI in news reporting must always be communicated to the public. This applies both to situations where technology has been used to help produce the news and to news where the source material has been created by machine intelligence.      This is equivalent to reporting the sample and the margin of error in news about polling.     STT guidelines on the use of AI for content creation  Early experimentation helps to understand AI  STT experimented with text generation using AI models several years ago, one of the first media houses in the world to do so. The language model used in the Scoopmatic project was based on STT's archive data. As the project progressed, it became clear that the self-learning language model needed more human guidance after all, as it was producing so much incorrect information. [...]  [...]"
ETHI31,TV2,Public Broadcaster,Denmark,Northern Europe,June,2023,DK,Position,https://sr.tv2.dk/media/giepaudc/etiske-retningslinjer-13-06-2023.pdf,,"Use of artificial intelligence (AI) As a starting point, TV 2 Danmark A/S wants to utilise any technology that can support, develop and improve our publishing work. This includes AI. The technology must never compromise or undermine our credibility. And any use must follow TV 2's data ethics rules. TV 2 is aware of the risks of new technologies, and use must be assessed to provide benefits for TV 2's programmes and our audience. Therefore, we continuously update our specific approach, guidelines and reservations in a special AI manual."
ETHI33,Usbek & Rica,News Media,France,Western Europe,August,2023,FR,Guidelines,https://usbeketrica.com/fr/article/notre-usage-de-l-ia-ce-a-quoi-usbek-rica-s-engage,,"Usbek & Rica undertakes... - not to publish any article or text generated by an artificial intelligence tool (even a news brief) without human supervision. We do, however, reserve the right to quote a few lines generated by such a tool as part, for example, of an article on the performance of artificial intelligence in writing (provided that such an article is written and edited by a Usbek & Rica journalist). We also reserve the right, on occasion, to use a tool such as ChatGPT as an interlocutor (along the lines of his interview on the best way to create religion published in the pages of our April 2023 issue). - not to publish images generated by artificial intelligence tools featuring identifiable human beings, whether living or dead. In other words, in the context of the generation of an image by an AI, we undertake not to write a prompt containing the name of a ""real"" person (all the more so if that person is famous), such as ""Mel Gibson dressed as a cowboy astride a cactus"". - not to publish images generated by artificial intelligence tools ""in the style of"", in order to preserve as far as possible the singularity of the work of each image and video creator, whatever their sector of activity. For example, we undertake not to write a prompt such as: ""Penguins eating cucumbers in a Wes Anderson-style film set"". - to always credit the artificial intelligence tools used to produce an image or text as fully and transparently as possible (in particular by specifying the name of the tool used and detailing the prompt created to generate an image). Finally, a word of clarification. We think it's rather presumptuous for a media outlet to claim to have definitively fixed the use it intends to make of AI to assist it in generating texts or images when this technique has only been available for a few months. These tools are bound to evolve, both technically and in terms of accessibility and above all the legal framework, which could lead us to update the commitments made above or to make new ones in the future. However, our editorial philosophy and approach will remain as set out above."
ETHI36,YLE,Public Broadcaster,Finland,Northern Europe,October,2023,EN,Principles,https://yle.fi/aihe/s/10005660,,"

Yle uses artificial intelligence to better fulfil its public service mission. We want to ensure that all audiences in Finland have equal opportunities to find Yle's content in a modern way.

We understand that artificial intelligence itself is not objective or neutral and that humans are always responsible. The AI solutions that Yle uses should never jeopardise the credibility of Yle. Therefore, we consider the following principles for responsible use of AI in all our operations and at all stages of our work.

Our AI solutions are guided by legality, fairness, equality and accessibility. It is important to Yle that the diversity of people, cultures and society at large is reflected in the development of AI. Therefore, the data and material we use to train AI needs to be such that it enriches our understanding of society and does not reinforce stereotypes. We also endeavour to become a role model and collaboration partner within the media industry and for other public service companies.

    A human is always responsible. A human is always responsible for the outcome when AI is used. When we develop content and services, it is clear who is responsible for the choices that AI makes. The liability for the content that AI produces is no different from other journalistic processes and the same requirements for reliability apply.

    We develop AI for the benefit of society, considering the environment. We strive to make the development, behaviour, and impacts of AI on society and the environment open, transparent and understandable for everyone. We understand that the computing requirements of AI also burden the environment and we make choices to minimise these impacts.

    We enrich the user experience, content and our work with AI. We are open to using AI when developing our content production, services and other operations. We boldly experiment with its possibilities to create new media experiences and update our working methods.

    Our use of AI is open, transparent and supports independent choices. Our goal is that both Yle employees and those who use Yle's services should have the opportunity to understand what the algorithms do and what data they use. We provide tools and opportunities for everyone to seek new perspectives and content themselves. Our AI solutions offer alternatives but do not obtrude.

    We protect people's personal privacy and data. We manage data in a transparent and responsible way. We protect people's information and personal privacy and explain how we utilise the material and data they have shared with us. We do not compromise on data integrity, quality or security.

    We respect copyright. When we use AI, we take copyright into account and the rights of people working on creative tasks. At the same time, we are aware that machine learning challenges existing compensation models for creative work. We seek new methods together with other players in the industry.

    We continuously evaluate, develop and update. We constantly assess AI related risks, the decisions made by algorithms and if the applications fulfil their purpose. We further develop our AI solutions and our own operations based on continuous evaluation. We are vigilant in monitoring and correcting biases and openly share our observations. We expect our partners to also act responsibly.


Yle uses artificial intelligence to better fulfil its public service mission. We want to ensure that all audiences in Finland have equal opportunities to find Yle's content in a modern way.

We understand that artificial intelligence itself is not objective or neutral and that humans are always responsible. The AI solutions that Yle uses should never jeopardise the credibility of Yle. Therefore, we consider the following principles for responsible use of AI in all our operations and at all stages of our work.

Our AI solutions are guided by legality, fairness, equality and accessibility. It is important to Yle that the diversity of people, cultures and society at large is reflected in the development of AI. Therefore, the data and material we use to train AI needs to be such that it enriches our understanding of society and does not reinforce stereotypes. We also endeavour to become a role model and collaboration partner within the media industry and for other public service companies.

    A human is always responsible. A human is always responsible for the outcome when AI is used. When we develop content and services, it is clear who is responsible for the choices that AI makes. The liability for the content that AI produces is no different from other journalistic processes and the same requirements for reliability apply.

    We develop AI for the benefit of society, considering the environment. We strive to make the development, behaviour, and impacts of AI on society and the environment open, transparent and understandable for everyone. We understand that the computing requirements of AI also burden the environment and we make choices to minimise these impacts.

    We enrich the user experience, content and our work with AI. We are open to using AI when developing our content production, services and other operations. We boldly experiment with its possibilities to create new media experiences and update our working methods.

    Our use of AI is open, transparent and supports independent choices. Our goal is that both Yle employees and those who use Yle's services should have the opportunity to understand what the algorithms do and what data they use. We provide tools and opportunities for everyone to seek new perspectives and content themselves. Our AI solutions offer alternatives but do not obtrude.

    We protect people's personal privacy and data. We manage data in a transparent and responsible way. We protect people's information and personal privacy and explain how we utilise the material and data they have shared with us. We do not compromise on data integrity, quality or security.

    We respect copyright. When we use AI, we take copyright into account and the rights of people working on creative tasks. At the same time, we are aware that machine learning challenges existing compensation models for creative work. We seek new methods together with other players in the industry.

    We continuously evaluate, develop and update. We constantly assess AI related risks, the decisions made by algorithms and if the applications fulfil their purpose. We further develop our AI solutions and our own operations based on continuous evaluation. We are vigilant in monitoring and correcting biases and openly share our observations. We expect our partners to also act responsibly.
"
ETHI37,France Info,Public Broadcaster,France,Western Europe,June,2023,FR,Position,https://www.francetvinfo.fr/replay-radio/le-rendez-vous-du-mediateur/l-utilisation-de-l-intelligence-artificielle-par-les-journalistes-a-franceinfo_5850095.html,,"According to a recent study by the World Press Organization, World Association of News Publishers (WAN-IFRA) in collaboration with Schickler Consulting, almost half of newsrooms around the world already use artificial intelligence tools. We know that journalists see it as a threat to their profession.  But we learn by reading this document that 70% of the participants in this survey welcome artificial intelligence rather favorably, and expect these tools to be useful to them in their daily lives as journalists. And only 20% of respondents indicated that they had directives from their editorial staff on the use of AI in their work, in terms of ethics, for example.  Emmanuelle Daviet: Listeners are starting to write to us on this subject, and one of them wants to know if artificial intelligence is used by journalists, and if so, what software is it?  Estelle Cognacq: So yes, we have been using software for several years. It is true that artificial intelligence has come to the forefront in recent months, but it is not new. It’s just that it’s a new artificial intelligence, as we say “conversational”, generative. We use other artificial intelligences, sometimes for five or six years, for the first ones. Generally, this is software that is there to assist our journalists, our editorial teams in the detection and ""sourcing"" of information, in the exploration of very large databases, and in the automatic transcription of audio. . These are the main uses.  For example, we use software called Trint for automatic audio transcription. That is to say, we will play an audio file, an interview, and it will transcribe the text. It’s a help for our journalists, it allows them to go faster, to then make summaries and use them. We use software called Data Miner, which allows us to detect information and events occurring through social networks, notably Twitter. So it detects, let's say, an explosion somewhere, a fire. And that allows our journalist to be alerted. It doesn't do the work for them at all.  In fact, once again, it's alert, and then, we have a partnership with a laboratory called Inria, on our true and false cell, for verifying information. It's a tool developed for us by them, called State Check, which helps us with fact-checking. This means that it will detect, among tweets from politicians or personalities, quantitative elements of figures on which we will then be able to work. And then, once we have chosen this sentence or this element to investigate, we have another part of the software which helps us to go into the Eurostat and INSEE databases, to find the real answer. .  What advantage do you see in the use of artificial intelligence in the work of journalists and what are its limits?  So first of all, the desired goal, for us, is not the automated production of content, that’s certain. It is rather to provide tools, technologies to our journalists, to free up time to devote themselves to parts of work which may have more value, more interest. And then there are things that are more complex to do: going by hand – a human – into databases of thousands of elements is not possible.  So the limits, then, we set them ourselves. Today, for example, we do not use software that generates images, videos or sound, nor ChatGPT, and all this generative artificial intelligence, as they say, today we look at it, we is in the process of seeing what we can do with it or not. But here we are, we are very careful. We have only one exception. In fact, it is to cover news relating to AI. And for educational explanation purposes, we could say: ""Look, for certain articles, we used ChatGPT"", we could say to ourselves: we take the baccalaureate dissertation for example, and say there you go, what is ChatGPT would have written? That's an example, but we'll always mention it.  Question from a listener: what measures and guarantees are put in place to ensure the ethics of the use of artificial intelligence by journalists?  I told you, we limited the uses. And above all, we are working on a charter to regulate this use and all these artificial intelligence tools. It must precisely guarantee respect for professional ethics, and great transparency on its use. We must tell our listeners, Internet users, readers: this is what we use let us or not as artificial intelligence software, to mention it, to make it known.  Furthermore, this charter must also be very, very responsive. The challenges are adapting to developments in these professions and these tools, because it’s very, very fast. We have to be very vigilant, follow what is being done, each time, question ourselves about what we use, and what we do not use. We have a big professional training challenge. It’s a world that’s changing very, very quickly. There may be the temptation to use these tools without safeguards, without asking questions. And we, at franceinfo and Radio France, want to support these innovations, that’s obvious. The world is changing, it is evolving. But on the other hand, wisely, and while respecting the values and ethics of a public service media."
ETHI38,Keystone - ATS,Press Agency,Switzerland,Western Europe,March,2023,FR,Position,https://www.keystone-sda.ch/documents/20143/0/Keystone-ATS+Positionnement+IA_v3_31-03-2023+(1).pdf/b4b48463-7b0f-c1ce-3d35-80172f0b5041?t=1681892117454,,"Position of the news agency regarding artificial intelligence Definition There is no general definition of artificial intelligence (AI). In principle, AI is a set of technologies (software) that can imitate human thought and/or action. For example, there are so-called weak AIs - which can only carry out a specific task - or complex neural AIs - which can make decisions themselves. This area includes machine learning, automated text generation, chatbots and others. Unless otherwise noted, Keystone-ATS herein includes all forms of artificial intelligence as well as automated content (“Lena”). This includes translation programs (like Deepl) and similar software. The possibilities offered by AI are a very dynamic area. Consequently, the agency must constantly adapt to these technologies. Position Keystone-ATS is in principle open to the opportunities provided by artificial intelligence, but is aware of the specific challenges, particularly with regard to ethical and legal issues. The agency therefore examines these new technologies with due care. In compliance with applicable law, ethical rules, guidelines relating to the “Declaration of Duties and Rights of the Journalist”, journalistic content can be produced and delivered to clients using AI. Keystone-ATS wants to use the advantages of artificial intelligence to help its customers and employees. Therefore, the use of an AI system must meet at least one of the following three criteria: - Added value for customers - Help for writing Keystone-ATS - Quality and accuracy of content assured Content: requirements quality and obligation of identification Keystone-ATS pays particular attention to the fact that the use of AI is transparent and can be traced and justified. The indication of sources is particularly important. The agency thus wishes to strengthen confidence in technology. When AI is used, Keystone-ATS applies the same quality controls as for traditional manual content creation (source verification, double-checking principle). 2/2 Keystone-ATS declares whether any form of AI was used to obtain information or content (for example, whether an AI reviewed and evaluated a dataset). Keystone-ATS reports whether any form of AI was used during content creation (see examples below). If content is translated using artificial intelligence (for example with Deepl or similar), this is not specifically stated. Keystone-ATS Partners ensures, wherever possible, that partner agencies work according to the same fundamental principles regarding artificial intelligence and apply comparable quality standards. If Keystone-ATS does not know the position of certain partners regarding AI, or if this is contrary to the principles of Keystone-ATS, the agency reserves the right to renounce the use of content from these partners . March 31, 2023 – alma/cojo/samu – Version 3.0 Examples of indication concerning content produced by Keystone-ATS Dispatch on the filling level of gas reservoirs/dam lakes: this dispatch was automatically generated on the basis of data from the Federal Office of Energy (SFOE) and the Gas Infrastructure Europe federation (GIE) and reread before distribution. Ice hockey – match results telegrams (identical to telegrams for handball): The telegrams are automatically generated with data provided by the Swiss Ice Hockey Federation and reread before broadcast. Results of the Swiss Loto and EuroMillions draws: this dispatch was produced with the support of Loterie Romande. It was automatically generated and proofread before broadcast. Covid-19 statistics: this dispatch was automatically generated based on FOPH data and reread before broadcast. Infographics (Lena): this infographic was produced automatically by the writing robot Lena. Images: (in the “Special Instructions” field) “Artificial intelligence used” or “AI used” or “Artificial Intelligence applied” or “AI applied”."
ETHI39,Mediahuis,Press Group,Belgium,Western Europe,June,2023,EN,Guidelines,https://www.independent.ie/editorial/editorial/aiframeworkguide140623.pdf,,"Augment, not replace
• AI should enhance journalism and support
our work.
• AI can handle time-consuming tasks, so you
can focus on important things.
• Our goal is to enhance the quality of our
journalism for our audience.
Transparency above all
• Always state when AI is used to create or
modify content.
• Publish AI guidelines and be transparent about
how we use AI.
• Encourage readers to give feedback and
let them review their data.
Human in the loop
• Don’t publish AI-made content without a person
checking the content or process.
• The editor-in-chief ensures AI technologies
comply with journalistic codes and standards.
• Designate a key contact for AI-related questions
and monitoring in the newsroom.
Be fair & without bias
• Watch out for biases in AI systems and work
to address them.
• Carefully balance journalistic, commercial,
and audience objectives when using AI.
Trust is key
• Remain trusted sources by verifying facts
and avoiding harmful AI-generated content.
• Help our readers to assess what and who
to trust.
• Respect copyrights, especially when AI imitates
a recognizable style or content.
Privacy & security as priority
• Prioritize data privacy and security when using
AI applications.
• Comply with privacy laws and where required
obtain user consent before using personal info.
• Explain how personal data is used in AI
applications and empower readers to control it.
AI training & skills
• Establish clear lines of accountability for AI
development and use.
• Train and qualify those responsible for AI
decisions.
• Invest in newsroom awareness and
communication about acceptable AI practices."
ETHI41,RTS,Public Broadcaster,Switzerland,Western Europe,May,2023,FR,Principles,https://www.rts.ch/entreprise/vos-questions/14071065.html/BINARY/Charte%20utilisation%20intelligence%20artificielle%20contenus%20%C3%A9ditoriaux.pdf,,"Charter concerning the use of artificial intelligence in
production of editorial content at RTS
Artificial intelligence (AI) is driving significant transformations in the
media field and RTS is obviously not immune to this fundamental movement. This
revolution profoundly modifies all facets of our work and its reception.
Eager to constantly adapt to changes in uses and modes of
work, also wishing to take advantage of technological advances, the
RTS has invested in this field for years both in the areas of production,
archiving and distribution of its contents.
If the RTS integrates AI into its practices, it does so by rigorously respecting the
professional ethics which guides it on a daily basis within the framework of its mandate of
public service. Respect for ethics helps maintain a relationship of trust,
more vital than ever, with the public.
Here are the fundamental principles that guide our current use of AI tools
generative. This charter may be subject to change, depending on future
developments in technology and its uses.
The principles defined in the charter are part of the general framework set by the SSR. There
this charter will thus be adapted or supplemented if necessary depending on the
provisions decreed by the SSR concerning the use of AI within the company.
General principles
Our use of artificial intelligence is not intended to produce content
fully automated. Its aim is to strengthen the capacities of professionals
enabling them to work more efficiently, so they can spend more time
time for high value-added tasks. AI must be understood as a tool that
does not replace but supports creativity, analysis, perspective or
again for example investigation.
ChatGPT and text generation models do not replace human expertise and
must be used as complementary tools to the analysis and expertise of
professionals.
Text generation & processing
The use of AI is accepted to summarize sources considered reliable by and
aimed at journalists for different media (social networks, titles for
sites, etc.) and audiences (children, young audiences, etc.)
The verification of sources and their traceability must be part of the practices in
force in the editorial offices. We do not publish any text, even partially processed
by AI, without human supervision.
Image generation
The RTS does not publish images generated by AI tools (Midjourney, Dall-E, etc.). THE
image generators on the market pose serious ethical problems both in terms of
issues related to bias and copyright. This does not correspond to the ethics
by the RTS. We monitor the market to detect whether tools allowing the creation
images/videos whose sources are verifiable and which respect copyright
were to emerge.
Transparency
The use of AI must be clearly reported to the public when it has an impact
significant on the production of content submitted to the public.
Cloning
As it stands, the RTS does not offer any cloning, except in the context of experiments.
strictly internal, whether audio (voices of presenters, journalists, etc.) or video
(avatars, deep fake, etc.). It may exceptionally be considered on the air
solely for the purposes of questions and reflections on the subject. In this case, it will be
clearly communicated to the public. The role of the public service and the trust it must maintain
in a context of dilution of reality is more important than ever. But cloning
can confuse and mislead the public. It is essential to position ourselves in a way
strong on these subjects."
ETHI42,Le Figaro,News Media,France,Western Europe,November,2023,FR,Charte,https://www.lefigaro.fr/medias/le-figaro-se-dote-d-une-charte-sur-l-usage-de-l-ia-generative-20231212,,"No AI-generated texts or photos

The result of the debates of a working group representative of the different editorial offices of Le Figaro (daily, Magazine, Madame Figaro, Figaro.fr) and their professions (editorial, publishing, iconography), this charter “puts in place safeguards very strong fools,” says Alexis Brézet. It thus stipulates that “Le Figaro does not intend to publish any article developed by generative artificial intelligence”, a principle which also applies to “photographs, illustrations, drawings and videos generated by AI”.

The spirit of the document, and the assurance provided to readers of Le Figaro, is that “all published content will continue to be produced and supervised by journalists from the editorial offices of Le Figaro. »

Le Figaro nevertheless does not reject artificial intelligence outright. Journalists will be able to use this technology in their preparatory work (synthesis of documents and databases, assistance with translation, etc.), before writing their articles. It can also help with layout or spelling correction, and can speed up the process of subtitling videos, or even their translation into other languages. But the group undertakes not to automate these processes.
Defense of copyright

Likewise, the editorial staff may be required to test speech synthesis software capable of reconstructing a human voice. This technology could be used to allow articles to be listened to in the voice of their authors, without the latter having to go behind a microphone. But nothing will be done without the authorization of the journalist nor without the reader being clearly informed that the voice he hears is synthetic. “Transparency is at the heart of this charter. We must not sow doubt, it is a moral contract with our readers,” insists Marc Feuillée.

The charter will evolve over the course of “the rapid changes in these innovations and the lessons that Le Figaro will draw from their use”, indicates the document. “This general framework establishes a precautionary principle in the face of dizzying technologies,” summarizes Alexis Brézet.

This document is also aimed at the designers of generative artificial intelligence, who have sucked up press content without authorization or financial compensation in order to train their models. “Le Figaro is viscerally attached to the defense of its copyright,” recalls the charter. The group recently modified the general conditions of its sites in order to refuse this indexing, as authorized by European copyright regulations. “Our message is very clear: our content is our property,” concludes Marc Feuillée."
ETHI43,BBC,Public Broadcaster,United Kingdom,Western Europe,February,2024,EN,Principles,https://www.bbc.co.uk/supplying/working-with-us/ai-principles/,,"Our BBC AI Principles are at the heart of our approach to using AI responsibly and apply to all use of AI at the BBC. They underpin the BBC’s public commitments about how we will use Generative AI.
Updated: 7 February 2024
We will act in the best interests of the public

BBC Values: Our use of AI will reflect the public service mission and values of our organisation: upholding trust; respect and inclusivity; boosting creativity; putting audiences at the heart of everything we do; being accountable and delivering quality; and working as one BBC.

BBC Editorial Values: When we use AI to create, present or distribute content we will make sure that this complies with the BBC’s editorial values, guidance and guidelines. 

Fairness: Our use of AI will be fair, equitable and inclusive for our audiences and staff. We will make sure that everyone has the opportunity to access and benefit from the range of content and services we provide and use.

Security & Robustness: We will use AI in a way that is secure, robust and safe. We will assess its accuracy and reliability, monitor its performance, document its use, and maintain continuity of service.
We will prioritise talent and creativity

Respecting Rights: Creators, contributors and suppliers play a vital role in our industry. We will consider the rights of creators, artists, contributors and rights holders when using AI. We will make sure our use of AI respects the data protection and privacy rights of individuals.

Human Creativity: We will use AI to support and empower human insight, talent and creativity.
We will be open and transparent

Transparency & Clear Explanations: We will be clear with audiences and staff about where we use AI and what data we collect. We will make sure they can understand why we use it, how it works and how it affects them.

Accountability: We will make sure that there is proper supervision of and clear accountability for our use of AI, including our use of content or services provided by others that involve AI.

Human Oversight: Our use of AI will be accompanied by effective and informed human oversight, including over the content or data involved as inputs and outputs."
ETHI44,Conseil de la presse suisse,Press Council,Switzerland,Western Europe,January,2024,FR,Guidelines,https://presserat.ch/fr/journalistenkodex/ia_lignesdirectrices/,,"Guidelines

Artificial intelligence (AI) brings new opportunities and challenges for journalism.

Generative AI programs, in particular, are able to produce various credible contents (text, image, sound) which induce a strong “reality effect”. Journalists must therefore exercise vigilance and restraint in using these programs to ensure reliable journalism.

The “Declaration of Duties and Rights of the Journalist” (hereinafter: “Declaration”) contains numerous guidelines that apply to the correct use of generative AI programs.

In this context, the Swiss Press Council notes:

1. Journalists and editorial staff are always ultimately responsible for respecting ethical standards for text, images, audio, video and other artificially generated material. Texts or text elements produced using AI programs must therefore only be used with full knowledge of the facts and must be strictly checked before their publication, according to the usual journalistic criteria (truthfulness, accuracy, reliability). The journalistic approach, with or without AI, requires the search for truth and requires correct treatment of sources.

     Figure 1 of the “Declaration” / guideline 1.1 (search for truth)
     Section 3 of the “Declaration” / guideline 3.1 (treatment of sources)

2. Content created using an artificial intelligence program must be marked as such. The Press Council recommends the greatest transparency in this matter. Newsrooms must explicitly state where and how they use AI tools.

3. The sources on which artificially generated content is based must be known, evaluated and mentioned to the same extent as for a “traditional” journalistic subject.

     Section 3 of the “Declaration” / guideline 3.1 (treatment of sources)

4. The images, sounds or videos thus produced must never mislead or cause confusion because of their resemblance to reality.

     Figure 4 of the “Declaration”

5. No confidential, personal or otherwise sensitive data should be introduced into AI programs without full control over the subsequent use of the data. It should be checked whether the use of personal data provided by AI programs is permitted.

     Figure 7 of the “Declaration”

6. Copyright must be respected, the content (texts, images, audio, videos and other elements) that an AI program takes from existing sources must be cited when they are published, according to the usual criteria.

     Figure 4 of the “Declaration” / guideline 4.7 (plagiarism)

The Press Council is aware of the very rapid developments in this area and will regularly review its guidelines as well as its directives (duties and rights), adapting them if necessary."
ETHI45,Le Monde,News Media,France,Western Europe,March,2024,FR,Principles,https://www.lemonde.fr/le-monde-et-vous/article/2024/03/13/le-groupe-le-monde-se-dote-d-une-charte-sur-l-intelligence-artificielle_6221734_6065879.html?lmd_medium=al&lmd_campaign=envoye-par-appli&lmd_creation=ios&lmd_source=twitter,,"The media landscape has experienced two revolutions in recent decades: the arrival of the Internet in the 1990s then the emergence of social networks in the 2000s. So-called generative artificial intelligence (AI) (capable of creating texts, images or videos) will it in turn upset a decidedly unstable ecosystem? The question has come up repeatedly in numerous debates, conferences, seminars and round tables organized since the launch of a general public version of the ChatGPT conversational robot at the end of 2022.

Without further ado, the Le Monde Group has decided to adopt a charter on the use of AI by its editorial staff, a text intended to complete its charter of ethics and professional conduct. The document was adopted on December 15, 2023 by the ethics and professional conduct committee of the Le Monde Group. Journalist societies from the various titles concerned (Courrier international, Le Monde, Télérama and La Vie) were involved in the discussions. This text should be statutorily adopted by the next general assembly of the Société Éditatrice du Monde.

The document is made up of four articles and establishes in the preamble a principle which has the merit of clarity: “In no case can AI replace humans in our journalistic productions and achievements. » This charter is aimed as much at the journalists of the press group as at those who read them. “It is essential to reaffirm to our readers the principles that govern us in order to maintain the confidence they place in the titles of the Le Monde Group,” he said.

Article 1 guarantees readers that no editorial content can be designed outside of the communities of human intelligence that constitute the editorial offices of the Le Monde Group. “Generative artificial intelligence (…) cannot in any way replace editorial teams,” he specifies. The use of generative AI is only authorized, under strictly regulated conditions, as a tool to assist editorial production. »

The charter on the use of AI within the Le Monde Group requires that any other use of generative AI be systematically and explicitly reported to the readership – “the use of generative AI to create images is prohibited” , it is also posed. One of the conditions for using generative AI is that it makes it possible to “improve the quality of information content”.
Treasures of ingenuity

Le Monde Group titles do not refrain from using certain tools using artificial intelligence. Le Monde in English is an example: this English-speaking version of the Le Monde site has offered every day since April 2022 several dozen articles produced by the editorial staff and translated into English. Each of these articles is subject to translation in three stages: the first “layer” of translation is carried out by the DeepL automatic translation software, the second by a professional translator (human!), the third by a English-speaking journalist.

This project would not have been possible without the improvement in the performance of translation software: the cost in working hours of a 100% human translation would have been dissuasive in view of the expected income. It is the progress of AI that has made it possible to believe that it could find its economic balance within a reasonable time. Ultimately, several jobs for English-speaking journalists were created within the editorial staff of Le Monde, so AI is not necessarily synonymous with job destruction.
Read also: Article reserved for our subscribers The media are cautiously trying out artificial intelligence

Another example of an additional service offered by Le Monde to its readers thanks to the controlled use of AI: the provision of an audio version of the articles. These “listening articles” are automatically generated from the written version of the text and the recorded voices of three actors and three actresses. Even if imperfections remain in the pronunciation of certain words and acronyms, they are gradually erased by the AI. Users of the Le Monde application now have access to this audio version for all articles.

Finally, AI helps the team that moderates comments from Le Monde subscribers to identify messages that do not respect our contribution charter. Here again, nothing can replace a human eye and a human brain to thwart the treasures of ingenuity that some people demonstrate in flirting with the limits defined by the rules set."
ETHI46,Nordic Journalism Network,Journalists Organisation,Sweden,Northern Europe,March,2024,EN,Guidelines,https://www.nordicaijournalism.com/ai-transparency,,"AI with ‘significant journalistic impact’ requires transparency
Just as the Swedish press ethics provide a basis for journalistic work, there
needs to be guidelines for when the use of AI requires transparency. We believe
that ""AI with a significant journalistic impact must be handled with openness and
clarity towards the audience"" is an appropriate such fundamental principle.
The principle is guiding rather than strictly defined. It is important that the
definition of what ‘significant journalistic’ means (and thus, the decision about
transparency) rests with the publisher, to ensure that it is adapted to the specific
needs and circumstances of each publication. We do not find it suitable or
desirable to define a fixed limit or type of AI-impact, but instead argue that the
responsibility for evaluation should be placed in the editorial process.
The principle applies to different media and content types, thus covering both
generative AI and other forms of AI tools and applications (e.g. when AI is used to
conduct investigative journalism).

Other internal AI-tooling does not require transparency
FUNDAMENTAL
Provided that the above principle is adhered to, we argue that public information
is not needed for internal applications where AI is used to support editorial or
commercial work. For such use cases, internal AI policies apply.
7Examples where AI can be used without actively sharing information with the audience:
Transcribing for internal use does not affect the content or credibility of the published
material. The focus should be on the finished result of the transcription rather than on
the method used to achieve it.
Proof-reading software is a common feature that does not affect the substance or
authenticity of the journalism. The goal is to deliver correctly spelled texts to the
audience, not which tools are used to achieve it.
SEO work is part of the editorial and commercial process to make content more visible
and accessible to a wider audience. It is the result of SEO strategies that are of interest
to the audience, rather than the technical methods used to implement them.
An AI implementation whose impact on journalism we did not reach the same
conclusion on in the different media companies is content recommendations. Some
argued that such do not require any active information effort towards the audience,
as they (in their standard form) only recommend content from the editorial
database and thus do not change the journalism. Others argued that they have a
significant impact on the journalistic product, and thus require transparency. The
example strengthens our recommendation (1) that each newsroom should make its
own journalistic assessment of what requires transparency.
3.
AI transparency must be approached as an iterative theme
Many of the AI technologies now used in the media industry are at an early stage,
during which it is necessary to be open and clear about their usage. This
openness will need to be reassessed over time as technology and user habits
and expectations evolve.
We are positive towards continued industry dialogue, and also see a great need
for media companies to have an ongoing internal dialogue on the topic. We
intend to follow up on our recommendations in six months, to evaluate their
continued relevance.
4.
Be specific about the type of AI tool applied
To counter audience perceptions of AI being an autonomous force operating
without editorial control, we encourage media companies to be specific when
telling our audiences what type of AI we used. This can serve an educational
function for both the media companies and the media audience, where we
demystify the concept of AI and instead communicate about the technology as a
tool.
We encourage the use of terms like “image generation” or “text analysis” to
more specifically describe the task which AI technologies have been part of
fulfulling.
Some media companies may not want or are unable to unduly favor AI
companies commercially by explicitly mentioning specific company names.
However, it is important to note that the terms of use and naming of model
names differ between different AI companies and models, and sometimes it may
not be possible to avoid mentioning model/company names. Therefore, where
journalistically relevant and copyright and/or contractually required, we believe
that an explicit naming of the models (commercial or open source) used is
appropriate. In addition to providing transparency to media consumers about our
processes/tools, this can also make it clear to our own organizations whether
we are utilizing a suitable range of AI tools/companies.
For most media aimed at the general public, we believe that technical details
(such as model versions and model weights) are redundant and may even be
counterproductive by making the consumer feel they do not understand the
technical jargon. However, we see a relevant exception in method descriptions
of editorial work, especially investigative journalism, where AI has a major
journalsitic impact.
10A editorial team has used AI to create a journalistic comic strip. The editorial team
believes that this has a significant journalistic impact and should hence inform
users that AI has been used. Therefore, in a caption, it states that ""the comic strip
is created with the support of an AI tool for image generation"".An editorial team has utilized a language model for investigative journalism and
concluded that AI usage has a significant journalistic impact by enabling data
analysis that would not be possible with the editorial team's human capabilities.
As the model in question is a crucial component in the publication, the team shares
information about the specific model they have used, and explains how method
choices such as prompts and temperature may have influenced the investigation.
Illustrative example
Illustrative example
5.
Share information in connection with consumed content
Where AI transparency is needed, information should be shared in connection
with the AI implementation. By sharing information alongside the consumed
journalistic content, we provide media consumers with a chance to understand
for themselves the extent to which AI is used with significant journalistic impact.
The recommendation applies to both AI as a tool (see the above example
regarding data analysis in investigative work) and for generative AI.
The recommendation is complicated by the fact that there isn't always a given
definition of what ""alongside"" means, especially for audio media. Here, we argue
that the fundamental recommendation (1) of ""significant journalistic impact"" is
central to the assessment work, and particularly the editorial teams'
responsibility to continuously evaluate the need for audience information. How
information reaches the audience can and should vary between publications.
11An editorial team uses AI to generate an article text and believes that this has a
significant journalistic impact as AI has made language and information selection.
Therefore, the editorial team should inform users that AI has been used.
Even though it is stated in the editorial team's AI policy that they sometimes use AI to
generate article texts, users must receive information about this specific usage
alongside the relevant content.
Illustrative example
6.
Harmonise the industry’s language around generative AI
We recommend a harmonised approach for describing AI within and between Swedish
media companies. This is mainly motivated by media consumers more readily having an
opportunity to understand and relate to AI in Swedish media if we communicate
similarily about its usage.
As a first step, we recommend a harmonized language regarding generative AI. As a
basis, we suggest the wording ""created with the support of"", to signal AI's actual impact
on the content and remind the media consumer that there is an editorial process (and
staff) behind the content. Alternative wordings that have been discussed but dismissed
include ""assisted by,"" ""together with,"" ""via,"" or ""by."" We believe these terms set incorrect
expectations and may diminish the perceived role of humans in the editorial process.
We envision the following foundation for more harmonised formulations: ""[the content
in definite form] is created with the support of an AI tool for [the task] ([the
model/tool], [the version]).""
12An editorial team publishes an AI-generated illustration and considers it to have a
significant journalistic impact. They inform their audience about it using one of the
following formulations:
The illustration is created with the support of an AI tool.
The illustration is created with the support of an AI tool for image generation.
The illustration is created with the support of an AI tool for image generation (DALL-E).
The illustration is created with the support of an AI tool for image generation (DALL-E, v.2).
Illustrative example
PRACTICAL
Depending on the specific media company's target audience or tone, a
combination/variant of these levels may be appropriate. Just like in several of our
other recommendations, this becomes an editorial decision.
13An editorial team publishes an AI-generated illustration and considers it to have a
significant journalistic impact. They inform as follows:
The illustration is created with the support of the AI tool DALL-E.
We recommend that individual media companies make their own assessments of
any additional information that wish to add. We expect that many media
companies will, perhaps on a case by case basis, choose to add a statement
about editorial quality control.An editorial team publishes an AI-generated article and considers it to have a
significant journalistic impact. They inform audiences using the following wording:
The article is created with the support of an AI tool for text generation and has
been reviewed by our editorial team before publication.
If Swedish media were to automate their content generation process (i.e., no
longer have employees writing prompts, reviewing generated content, and
manually publishing), the above formulations would naturally be reconsidered, as
the content would no longer be created with the support of AI, but rather,
created by AI.
We also believe that there is good reason to consider whether the term ""AI""
should remain in the formulation for long, but assess that it is relevant at present.
Therefore, like everything else in these recommendations, the formulation needs
to be continuously evaluated and updated where/when relevant.
Illustrative example
Illustrative example
7.
Avoid visual labels (icons) for AI in editorial media
We advise against standardised visual labeling (e.g., an icon) for AI in editorial
media. We are motivated by three main arguments:
An explicit visual differentiation may give the impression that the credibility of
AI-generated/affected content overall differs from content created by human
journalists or editors. The foundation must always be that if the content were
not credible, it would not be published by our editorial media.
Visual labeling may be challenging to implement consistently across different
media platforms and content types. There can be many different ways to use
AI in media production, and attempting to create a general visual label
covering all these use cases can be complicated for media companies and
difficult for our users to understand.
It risks becoming a cumbersome process given where we are in AI
development. New tools and use cases will emerge along the way, and a
visual standard risks quickly becoming irrelevant.
This recommendation follows a discussion about whether it is appropriate for
editorial media to use photorealistic AI-generated images, especially in news
feeds. Recommendations regarding this are beyond the scope of this project ,
but we still want to emphasize that the recommendation would need to be
reassessed for this specific case if any of the Swedish media were to start
publishing photorealistic AI illustrations.
4. Be specific about the type of AI tool applied
5. Share information in connection with consumed content
6. Harmonise the industry's language around generative AI
7. Avoid visual labels (icons) for AI in editorial media
1. AI with 'significant journalistic impact' requires transparency
2. Other internal AI-tooling does not require transparency
3. AI transparency must be approached as an iterative theme."
ETHI47,AJP,Professional Organisation,Belgium,Western Europe,March,2024,FR,Principles,https://www.ajp.be/deontologie-recommandations/,,"TRANSPARENCY CLEAR IDENTIFICATION. All editorial content generated by artificial intelligence must be identified as such, on all media through which this content is distributed. This is particularly important in a context of mistrust towards the media. INTERNAL VALIDATION. All internal recommendations on the use of artificial intelligence must be validated by a body representative of the editorial staff (company of editors, journalists, etc. or an ad hoc committee in their absence). TOTAL TRANSPARENCY. All internal recommendations must be public, detailed and consultable via various channels and understandable to the general public. LIABILITY EDITORIAL RESPONSIBILITY GUARANTEED. Journalists are responsible for the content they publish, but final responsibility lies with the editor-in-chief. These guarantee compliance with the principles of the Code of Ethics in the event of the development of systems partially or entirely driven by artificial intelligence. Any use of AI for the production of information content must be validated by the editor-in-chief, the information department or at least, a department head. Furthermore, systematic control must be carried out before and after publication, in a human-AI-human process. THE IMPORTANCE OF TRAINING. If generative artificial intelligence can be a tool for journalists, its use must be mastered. The AJP encourages editorial managers to organize ongoing training for editorial staff (employed and independent journalists, web editors, picture editors, infographic designers, etc.). The AJP also invites journalism schools to integrate training for students into the educational system. ETHICS ABSOLUTE RESPECT FOR SOURCES. No confidential or potentially confidential data and no information protected by source secrecy may be transmitted to an AI. EXTREME CAUTION. Journalists exercise caution when using AI: its use does not guarantee the truthfulness, reliability, quality, authenticity or impartiality of this content. This is the result of algorithms which are not neutral, lack nuance, creativity and emotion and whose goal is not the search for truth but to summarize other content. Any content produced by generative AI must be treated as an unverified source of information. As a reminder, generative AI does not specify its sources. In addition, if there is doubt about the authenticity of information generated by AI, journalists and editorial staff avoid disseminating it. AI IN THE SERVICE OF INFORMATION. The use of AI must present an advantage over its non-use in improving the reliability, veracity and quality of information. AJP RECOMMENDATIONS FOR TRANSPARENT, RESPONSIBLE AND ETHICAL USE OF ARTIFICIAL INTELLIGENCE In a context where artificial intelligence is increasingly used in newsrooms, the AJP considers that strict and clear guidelines must be taken. Its Board of Directors has established recommendations for journalists and editorial managers. These primarily concern generative artificial intelligence making it possible to generate texts, visuals, videos and audio that are truer than life. As specified in the Paris Charter on AI and Journalism, the use and development of AI in the journalistic sector must be dominated by the key values of journalistic ethics: truthfulness, accuracy, fairness, impartiality, independence, non-harm, non-discrimination, accountability, respect for privacy and confidentiality of sources. In view of technological progress, this charter is intended to be progressive. The AJP also invites its members to inform it of developments and developments in the use of AI in editorial offices. BEWARE OF STEREOTYPES. Journalists are careful not to disseminate stereotypes and comments contrary to anti-discrimination legislation that AI is likely to reproduce. As a reminder, the AJP and the CDJ have published recommendations specific to certain social subjects such as LGBTQIA+ issues, foreign people and people of foreign origin, etc. ONE USE IS NOT THE OTHER. The alteration of photos, videos or sounds modifying the initial meaning of information is prohibited. The use of images (photographs, press cartoons, animated images) generated by AI should also be prohibited, unless they are the subject of a news article. In this case, it must be clearly stated that these illustrations were produced by AI. More generally, the AJP notes that several editorial staff have already positioned themselves on the good and bad uses of AI and therefore do not put all technological tools on the same footing, thus differentiating AI producing from AI content as an aid to journalists. Among the “good” uses listed by several international media: translation of texts, production of short texts for social networks, suggestion of titles, search for subjects and themes, improvement of Google referencing, suggestion of article plans or titles , generation of interview questions, summary of old reports, creation of infographics or data visualization. Among the “bad” ones, which some media have also banned: texts and images generated by AI and the reproduction of voices. COPYRIGHT AN INCOMPLETE LEGAL FRAMEWORK. In terms of copyright, generative AI poses numerous problems which are not yet met by the legal framework, or are only partially met: use by harvesting of works protected by copyright, without the knowledge of the authors (for example, databases of press articles and photographs), transformation of these source works into new works, without the consent of the authors or rights holders, non-respect of moral rights attached to the source works (authorship/maternity, contextual integrity, etc.), etc. The ownership of rights to works produced partially or totally by AI is also debated. Media and journalists must be attentive to the multiple copyright aspects raised by the use of generative AI tools. Pending the finalization of a legal framework at European or national level, the AJP invites journalists to exercise the greatest caution in this matter. JOB SECURITY GUARANTEED JOB MAINTENANCE. Task automation can have a significant impact on the newspaper industry. AI cannot be a pretext for eliminating jobs. If this automation frees up working time for journalists, it must be used to produce quality information. Conversely, artificial intelligence can be an opportunity for job creation. For example, new fact-checkers will undoubtedly be needed. Regardless, AI cannot replace journalists, editors and photographers. Even if its use modifies certain tasks or eliminates others, this does not make media workers redundant."
ETHI48,Mediafin,News Media,Belgium,Western Europe,December,2023,FR,Principles,https://www.lecho.be/dossiers/intelligence-artificielle/intelligence-artificielle-et-journalisme-la-charte-de-l-echo-et-du-tijd/10508789.html,,"Faced with the development of artificial intelligence, L’Écho and De Tijd are committed to guaranteeing transparency, human intervention and responsibility in their journalistic practice.
The trust of our readers is our most precious asset, so we do everything we can to only publish reliable information, that is to say verified and cross-checked. To do this, we impose the highest quality standards on ourselves and respect the strictest ethical rules.
At L’Écho and Le Tijd, we are enterprising by nature, so we are logically interested in the opportunities offered by artificial intelligence (AI). We are also aware of the pitfalls and risks that this technology represents. This is why we have established a charter that frames our approach to AI, giving priority to transparency, human control and strict compliance with journalistic ethics.
This charter aims to help us make the best choices in terms of AI. As technology is constantly evolving, this set of rules must be reviewed and updated regularly.
The 6 pillars of the use of AI at L’Écho and Tijd
1- JOURNALISTS ALWAYS HAVE THE LAST WORD
Journalism - Our journalistic practice remains, in essence, a human activity: the collection of information, its verification and the way of transmitting it are the work of professionals. The sole purpose of using AI technology is to support this human process. The journalist and the editor-in-chief remain entirely responsible for the accuracy of the information published. Nothing appears without having been read, reread and validated by a journalist and an editor.
Each journalistic story from L’Écho complies with our professional and ethical principles and standards. This requirement also applies to articles that use AI. L’Écho only publishes an article if the editorial team has ensured the relevance and reliability of the AI technology used.
The editorial staff of L’Écho and Le Tijd are constantly looking for new ways to improve their journalistic work. If AI applications can help us, we adopt them.
Distribution of information - We can use algorithms to optimize the digital distribution of our journalistic production, whether in newsletters, positions on the site or in the L'Écho application. We also make personalized content recommendations, for example in the “Selected for you” section and the “L’Écho Dimanche” newsletter. These algorithms are continuously tested and evaluated.
2- A GUARANTEE OF TRANSPARENCY ON THE USE OF AI
Every time we use AI in the journalistic creative process, we make it clear. Not only for texts or summaries of journalistic stories, but also for the production of podcasts, videos, illustrations or other graphic elements.
This approach does not apply to routine tasks that have no impact on the journalistic creation process, such as translation and transcription.
3- AI, TO IMPROVE OUR JOURNALISM
The editorial staff of L’Écho and Le Tijd are constantly looking for new ways to improve their journalistic work. If AI applications can help us, we adopt them.
We can use it, for example, to translate a working document or carry out research in a mass of documents in order to have more time to analyze them in depth and identify the most relevant trends. Before validating and using AI tools, we carefully test and evaluate them.
4- WE KEEP A CRITICAL VIEW
Reliability is at the heart of our editorial promise. The principles of cross-checking and double-checking remain the foundation of information collection. Given the rise of raw information generated by AI in all areas, we are increasing our vigilance in order to be able to guarantee the authenticity of the sources consulted at all times.
We are aware of possible biases, stereotypes or hallucinations created by AI tools and remain vigilant in this regard.
5- WE ALWAYS RESPECT PRIVACY, SECURITY AND COPYRIGHT
It goes without saying that, for any use of AI, the editorial staff always complies with legislation on the protection of privacy and respects the intellectual property rights of the material used. 6- A CONSTANT CONCERN FOR TRAINING IN AI TOOLS
L’Écho organizes training for editorial staff. These training courses focus on ethical aspects, tools and the impact of AI on society as a whole.
Members of the editorial staff are also invited to contact a contact person at the editorial staff whenever they have questions about one or another use of AI.
AI is also a central journalistic theme for L’Écho. This technology is omnipresent in business, society, our professional and personal lives. This is a democratic issue that requires vigilance and critical thinking.
As journalists, and to inform you, we must understand how AI works, what risks are involved and what its impact is in all dimensions of our lives.
The editorial staff of L’Écho and Le Tijd undertake to respect this charter.
This charter constitutes a document that will be reviewed and updated regularly, since AI and its applications are also constantly evolving."
ETHI49,Blick,News Media,Switzerland,Western Europe,April,2024,DE,Guidelines,https://www.blick.ch/community/diese-redaktionellen-leitlinien-gelten-bei-uns-so-machen-wir-beim-blick-journalismus-id19639702.html,,"Artificial Intelligence (AI)

Blick takes full responsibility for all published content – regardless of whether it was created by a human or an AI.

The results generated by AI tools are critically questioned by Blick employees. The information is verified, checked and supplemented using our own judgment and specialist knowledge.

Based on Ringier AG's AI guidelines, Blick labels content created with AI tools. Labeling is not necessary if an AI tool is only used as an aid (for example, for translation, converting audio to text, or proofreading).

When using AI tools, Blick ensures that no copyrights, personal rights, data protection regulations, confidentiality obligations or other rights and laws are violated.

The AI tools and technologies developed, integrated or used by Ringier should be fair, impartial and non-discriminatory. For this reason, our own AI tools, technologies and integrations are regularly reviewed and adjusted if necessary."
ETHI50,Roularta,News Media,Belgium,Western Europe,September,2023,FR,Principles,https://trends.levif.be/301360-2/,,"AI: editorial charter of Roularta Media Group (RMG)

Our commitments regarding the use of generative artificial intelligence within the RMG editorial staff.

Artificial intelligence, already used in our titles as an automation tool (machine learning, recommendations, targeting, etc.), has recently reached a new level with the advent of generative artificial intelligence (GAI) models. This is a major technological upheaval, both in terms of the scope of the sectors it will affect and the speed of its adoption by professionals and the general public.

This technological development offers many opportunities and raises many ethical questions, particularly regarding its use in journalism. It therefore seems necessary to us to define as precisely as possible the framework within which these technologies must or must not be used within the Roularta Media Group editorial offices.

Faithful to the relationship of trust that binds us to our readers, we have chosen to publish these principles. We also encourage our readers to send us their questions and comments which we will treat with the greatest attention. To do this, you can send an email to the respective editor.

All our editorial publications are synonymous with quality, reliable and relevant information. The continuity of this position is essential to guarantee the accuracy of information and must remain our compass. It is the quality of the work of our journalists, their ability to find original angles, to decipher the news and to provide added editorial value that makes our brands successful.

Aware that the areas of application of AI will continue to expand and that its importance is set to grow over time, we will update and clarify our position based on the ethical charters and practices applicable to the various publications of our band.

Our commitments cover two categories of use of AI:
Using text generators

We do not intend to publish editorial content generated in whole or in part by AI without editorial and human supervision. However, we reserve the right to do so for illustrative purposes (for example as part of a ChatGPT study). In this case, we will explicitly indicate the origin of the content. We also reserve the right to use AI tools for the purpose of summarizing original articles produced by a journalist.

We also reserve the right to use AI as an enrichment, research or synthesis tool to help and fuel the work of journalists. Journalists can use these tools as a search engine, for example, but must always return to their own sources to guarantee the origin of their information.
Use of sound, image or video generators

We do not intend to exclusively publish AI-generated images or videos without editorial and human oversight. As with all editorial content, we want to guard against any risk of error or copyright infringement.

As for voice AI, it will only be used in the context of a journalistic article.

AI-generated images are used to illustrate some of our articles. These are only illustrations easily identifiable as such, and not photographs. In all cases, we will ensure that the provenance is explicitly indicated to the reader.

However, as with texts, we can use AI as a tool to summarize videos, in particular to facilitate their distribution on different platforms."
ETHI51,Der Spiegel,News Media,Germany,Western Europe,June,2023,DE,Principles,https://devspiegel-medium-com.translate.goog/k%C3%BCnstliche-intelligenz-und-der-journalismus-wie-wir-beim-spiegel-dar%C3%BCber-denken-c83ee5c68965?_x_tr_sl=de&_x_tr_tl=en&_x_tr_hl=fr&_x_tr_pto=wapp&_x_tr_hist=true,,"Artificial intelligence and journalism: how we think about it at SPIEGEL
The rapid development of artificial intelligence (AI) has the potential to fundamentally change our society - and with it journalism. Individual media outlets are already publishing content created by AI. Others, like SPIEGEL and manager magazin, rely on AI tools in production or use AI-generated images, each transparently declared, as part of their reporting. At the same time, the debate about possible regulation is in full swing.
We in the SPIEGEL Group want to use artificial intelligence sensibly to improve the offering for users and to make our internal processes more efficient. At the same time, we are committed to our journalistic standards and principles, because the trust of our readers in the SPIEGEL brand is our most important asset. This text takes a look at the challenges that a company like SPIEGEL faces when using AI, the guidelines that we want to follow and the question of how the tech platforms and media companies can work together constructively. to use AI sensibly for society, to strengthen the role of independent media and thus to secure and strengthen the trust of users.
In the last few weeks we have therefore been dealing with artificial intelligence at very different levels of the company. One of the results of the discussion are guidelines that we want to follow in editorial, documentation, product & sales but also in the other areas of the group.
The principles for the use of artificial intelligence in the SPIEGEL Group
•	Artificial intelligence (AI) will help us improve our offering for our users and simplify our work. We see potential for improving our processes, particularly in automation, while the opportunities for SPIEGEL with AI, which generates content from scratch, are less clear and the potential risks are greater. In any case, we want to approach the possibilities that AI systems offer with great openness and encourage our teams as well as individual employees to deal with them.
•	The trust of our users in our journalism is our most important asset. As with any technology we use, we are responsible for all of our content. SPIEGEL therefore always has control over everything we publish - either by re-approving texts generated or edited using AI or at least by clearly defining the criteria for text generation and modification.
•	We never use artificial intelligence in our own products without supervision or clear rules and check whether the result meets our strict standards of factual accuracy, balance and critical independence. We use testing mechanisms and approvals to ensure in particular that the so-called “hallucinations” of generative AI are not published. If artificial intelligence systems play a significant role in the preparation or presentation of textual content, we always declare this transparently and clearly for readers. The same applies to images that were created or significantly modified using artificial intelligence.
In addition to the “big” questions about how artificial intelligence will change the media landscape and our society and where the opportunities lie for a house like SPIEGEL, there are also very pragmatic questions. What data do we want to feed into AI models? Where are the legal and ethical red lines? And the question of all questions: Who in the company can purchase software licenses? We have also developed common guidelines on these issues in order to ensure data protection, a discussion of ethical issues surrounding artificial intelligence and an exchange within our company.
•	We do not copy sensitive personal or company data into AI systems that have not undergone data protection and security checks.
•	When selecting artificial intelligence systems, we ensure that they comply with the applicable legal framework and that the providers are prepared to seriously engage in ethical debates about AI
•	When we use service providers who work with AI, we make sure that they meet the two criteria mentioned above.
•	We document our tests and experiences with AI and exchange ideas within the company, with other media and with other partners.
Last but not least, it is clear to us: Due to the high pace of AI development, it is impossible to conclusively assess all the opportunities and risks today. We will therefore continually adapt the principles to the specific risks and opportunities.
Looking inwards is not enough - how do we work together constructively?
However, it is not enough to ask yourself how media can use artificial intelligence to improve their offerings. The effects of the revolution that will affect us in the next few years extend far beyond this internal view of a media company. The principles of transparency and openness, coupled with an awareness of the responsibility associated with the use of artificial intelligence and mutual learning, also apply to collaboration between major tech platforms and media across the digital ecosystem.
That's why we asked ourselves early on how the large technology platforms, which not only have a significant influence on the development of artificial intelligence, but above all also its use in our everyday lives, and how we as media companies can work together constructively. It is just as important to prevent the media from hiding behind a purely regulatory and ancillary copyright debate as we all have an interest in ensuring that the large technology platforms, as drivers of innovation, are aware of their special responsibility in the use of artificial intelligence.
The exchange between the technology and publishing industries is therefore of great importance in order to protect the interests of both sides. The joint development of ethical guidelines and mutual support in implementation can create a basis for a sustainable partnership. Among other things, we have the chance to
•	work together towards the goal of an informed society,
•	to establish ethical frameworks to counteract disinformation and improve the quality of information,
•	to identify opportunities and risks early through joint experimentation,
•	to strengthen the role of journalism in our democracies by respecting its special importance,
•	To maintain respect for the rights of all partners so as not to damage the relationship between technology companies and publishers,
•	to ensure fair linking to the underlying content in order to improve the quality of AI-controlled offers,
•	ensure responsible and high-quality content mining to advance the development of AI capabilities,
•	to rely on open technological solutions to provide reliable information,
•	and to create trust in artificial intelligence as well as enable its further development through transparency and feedback options.
The debate about such a framework for cooperation between media companies and technology platforms should be conducted between individual companies as well as as an industry. We at SPIEGEL want to approach this debate openly and, in addition to the possibilities of artificial intelligence for our company, we also want to keep an eye on the opportunities and risks for the media industry as a whole.
The focus is on our sense of responsibility for the SPIEGEL, manager magazin and Harvard Business manager brands and journalism. The internal principles for the use of AI in the SPIEGEL Group serve as guidelines to ensure that AI systems are used responsibly and meet journalistic standards and principles. And through dialogue and collaboration between technology companies and publishers, we can create an information landscape based on trust, quality and ethics.
"
